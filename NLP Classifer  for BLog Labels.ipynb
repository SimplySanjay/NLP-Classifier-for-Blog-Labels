{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a> **DOMAIN**: Digital content management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a> **CONTEXT**: Classification is probably the most popular task that you would deal with in real life. Text in the form of blogs, posts, articles, etc.are written every second. It is a challenge to predict the information about the writer without knowing about him/her. We are going to create a classifier that predicts multiple features of the author of a given text. We have designed it as a Multi label classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a> **DATA DESCRIPTION**: Over 600,000 posts from more than 19 thousand bloggers The Blog Authorship Corpus consists of the collected posts of 19,320 bloggers gathered from blogger.com in August 2004. The corpus incorporates a total of 681,288 posts and over 140 million words - or approximately 35 posts and 7250 words per person. Each blog is presented as a separate file, the name of which indicates a blogger id# and the blogger’s self-provided gender, age, industry, and astrological sign. (All are labelled for gender and age but for many, industry and/or sign is marked as unknown.) All bloggers included in the corpus fall into one of three age groups:\n",
    "    \n",
    "• 8240 \"10s\" blogs (ages 13-17),\n",
    "    \n",
    "• 8086 \"20s\" blogs(ages 23-27) and    \n",
    "• 2994 \"30s\" blogs (ages 33-47)\n",
    "    \n",
    "• For each age group, there is an equal number of male and female bloggers. Each blog in the corpus includes at least 200 occurrences of common English words. All formatting has been stripped with two exceptions. Individual posts within a single blogger are separated by the date of the following post and links within a post are denoted by the label url link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a> **PROJECT OBJECTIVE**: To build a NLP classifier which can use input text parameters to determine the label/s of the blog. Specific to this case study, you can consider the text of the blog: ‘text’ feature as independent variable and ‘topic’ as dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Read and Analyse Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\sanja\\\\OneDrive\\\\Desktop\\\\PGP AIML\\\\AI - Topics\\\\4. Natural Language Processing\\\\Project\\\\NLP- Project 1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'blogs.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.unpack_archive(filename, extract_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the blogs.csv file\n",
    "\n",
    "data = pd.read_csv('blogtext.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 681284 entries, 0 to 681283\n",
      "Data columns (total 7 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   id      681284 non-null  int64 \n",
      " 1   gender  681284 non-null  object\n",
      " 2   age     681284 non-null  int64 \n",
      " 3   topic   681284 non-null  object\n",
      " 4   sign    681284 non-null  object\n",
      " 5   date    681284 non-null  object\n",
      " 6   text    681284 non-null  object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 36.4+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>14,May,2004</td>\n",
       "      <td>Info has been found (+/- 100 pages,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>13,May,2004</td>\n",
       "      <td>These are the team members:   Drewe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>In het kader van kernfusie op aarde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>testing!!!  testing!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>11,June,2004</td>\n",
       "      <td>Thanks to Yahoo!'s Toolbar I can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>10,June,2004</td>\n",
       "      <td>I had an interesting conversation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>10,June,2004</td>\n",
       "      <td>Somehow Coca-Cola has a way of su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>10,June,2004</td>\n",
       "      <td>If anything, Korea is a country o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>10,June,2004</td>\n",
       "      <td>Take a read of this news article ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>09,June,2004</td>\n",
       "      <td>I surf the English news sites a l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id gender  age              topic      sign          date  \\\n",
       "0  2059027   male   15            Student       Leo   14,May,2004   \n",
       "1  2059027   male   15            Student       Leo   13,May,2004   \n",
       "2  2059027   male   15            Student       Leo   12,May,2004   \n",
       "3  2059027   male   15            Student       Leo   12,May,2004   \n",
       "4  3581210   male   33  InvestmentBanking  Aquarius  11,June,2004   \n",
       "5  3581210   male   33  InvestmentBanking  Aquarius  10,June,2004   \n",
       "6  3581210   male   33  InvestmentBanking  Aquarius  10,June,2004   \n",
       "7  3581210   male   33  InvestmentBanking  Aquarius  10,June,2004   \n",
       "8  3581210   male   33  InvestmentBanking  Aquarius  10,June,2004   \n",
       "9  3581210   male   33  InvestmentBanking  Aquarius  09,June,2004   \n",
       "\n",
       "                                                text  \n",
       "0             Info has been found (+/- 100 pages,...  \n",
       "1             These are the team members:   Drewe...  \n",
       "2             In het kader van kernfusie op aarde...  \n",
       "3                   testing!!!  testing!!!            \n",
       "4               Thanks to Yahoo!'s Toolbar I can ...  \n",
       "5               I had an interesting conversation...  \n",
       "6               Somehow Coca-Cola has a way of su...  \n",
       "7               If anything, Korea is a country o...  \n",
       "8               Take a read of this news article ...  \n",
       "9               I surf the English news sites a l...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(681284, 7)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There are 681284 records and it's huge to perform data analysis and computational task on such huge volumes of data. Hence, we are going take subset of the data and re-run with entire dataset once all errors are fixed and optimization is done. We will use 50k records only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean the Structure data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_df = pd.read_csv('blogtext.csv', nrows = 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id        0\n",
       "gender    0\n",
       "age       0\n",
       "topic     0\n",
       "sign      0\n",
       "date      0\n",
       "text      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No Missing values in the blog dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminate Non-English textual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "Requirement already satisfied: six in c:\\users\\sanja\\anaconda3\\lib\\site-packages (from langdetect) (1.15.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993227 sha256=b7df0929b53db2c27a9a8d62226ad42e86198ca6eedfa49388b3b8a241c9f36e\n",
      "  Stored in directory: c:\\users\\sanja\\appdata\\local\\pip\\cache\\wheels\\13\\c7\\b0\\79f66658626032e78fc1a83103690ef6797d551cb22e56e734\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n"
     ]
    }
   ],
   "source": [
    "! pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "def detect_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_df = blog_df[blog_df['text'].apply(detect_english)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 47726 entries, 0 to 49998\n",
      "Data columns (total 7 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      47726 non-null  int64 \n",
      " 1   gender  47726 non-null  object\n",
      " 2   age     47726 non-null  int64 \n",
      " 3   topic   47726 non-null  object\n",
      " 4   sign    47726 non-null  object\n",
      " 5   date    47726 non-null  object\n",
      " 6   text    47726 non-null  object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 2.9+ MB\n"
     ]
    }
   ],
   "source": [
    "blog_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Preprocess unstructured data to make it consumable for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2A. Eliminate All special Characters and Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>14,May,2004</td>\n",
       "      <td>Info has been found (+/- 100 pages,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>In het kader van kernfusie op aarde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>testing!!!  testing!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>11,June,2004</td>\n",
       "      <td>Thanks to Yahoo!'s Toolbar I can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>10,June,2004</td>\n",
       "      <td>I had an interesting conversation...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id gender  age              topic      sign          date  \\\n",
       "0  2059027   male   15            Student       Leo   14,May,2004   \n",
       "2  2059027   male   15            Student       Leo   12,May,2004   \n",
       "3  2059027   male   15            Student       Leo   12,May,2004   \n",
       "4  3581210   male   33  InvestmentBanking  Aquarius  11,June,2004   \n",
       "5  3581210   male   33  InvestmentBanking  Aquarius  10,June,2004   \n",
       "\n",
       "                                                text  \n",
       "0             Info has been found (+/- 100 pages,...  \n",
       "2             In het kader van kernfusie op aarde...  \n",
       "3                   testing!!!  testing!!!            \n",
       "4               Thanks to Yahoo!'s Toolbar I can ...  \n",
       "5               I had an interesting conversation...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to eliminate all special characters and numbers\n",
    "\n",
    "import re\n",
    "\n",
    "blog_df.text = blog_df.text.apply(lambda x: re.sub('[^A-Za-z]+', ' ', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2B. Lowercase all textual data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_df.text = blog_df.text.apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2D. Remove all extra white spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_df.text = blog_df.text.apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2C. Remove all Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=set(stopwords.words('english'))\n",
    "blog_df.text = blog_df.text.apply(lambda t: ' '.join([words for words in t.split() if words not in stopwords]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30389</th>\n",
       "      <td>3458177</td>\n",
       "      <td>female</td>\n",
       "      <td>24</td>\n",
       "      <td>Banking</td>\n",
       "      <td>Aries</td>\n",
       "      <td>08,August,2004</td>\n",
       "      <td>upon time tkgs actually group girls became fri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44329</th>\n",
       "      <td>3682212</td>\n",
       "      <td>female</td>\n",
       "      <td>27</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Leo</td>\n",
       "      <td>01,July,2004</td>\n",
       "      <td>tax refund came today hooray yes yes know thou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45283</th>\n",
       "      <td>2383253</td>\n",
       "      <td>female</td>\n",
       "      <td>16</td>\n",
       "      <td>Student</td>\n",
       "      <td>Virgo</td>\n",
       "      <td>11,April,2004</td>\n",
       "      <td>yes survived spring break remember</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2616</th>\n",
       "      <td>589736</td>\n",
       "      <td>male</td>\n",
       "      <td>35</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Aries</td>\n",
       "      <td>05,August,2004</td>\n",
       "      <td>agree points disagree others thanks dialogue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13949</th>\n",
       "      <td>480727</td>\n",
       "      <td>male</td>\n",
       "      <td>23</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Pisces</td>\n",
       "      <td>02,June,2004</td>\n",
       "      <td>gotta lax oneself juz erm gonna call issit p g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  gender  age       topic    sign            date  \\\n",
       "30389  3458177  female   24     Banking   Aries  08,August,2004   \n",
       "44329  3682212  female   27      indUnk     Leo    01,July,2004   \n",
       "45283  2383253  female   16     Student   Virgo   11,April,2004   \n",
       "2616    589736    male   35  Technology   Aries  05,August,2004   \n",
       "13949   480727    male   23      indUnk  Pisces    02,June,2004   \n",
       "\n",
       "                                                    text  \n",
       "30389  upon time tkgs actually group girls became fri...  \n",
       "44329  tax refund came today hooray yes yes know thou...  \n",
       "45283                 yes survived spring break remember  \n",
       "2616        agree points disagree others thanks dialogue  \n",
       "13949  gotta lax oneself juz erm gonna call issit p g...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a base Classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will drop Id and Date columns as they are not useful for model building\n",
    "\n",
    "blog_df.drop(labels =['id', 'date'], axis =1, inplace =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we want to make this into a multi-label classification problem, we need to merge all the label columns together, so that we have all the labels together for a particular sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-label column\n",
    "\n",
    "blog_df['labels'] = blog_df.apply(lambda col: [col['gender'], col['age'], col['topic'],col['sign']], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>info found pages mb pdf files wait untill team...</td>\n",
       "      <td>[male, 15, Student, Leo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>het kader van kernfusie op aarde maak je eigen...</td>\n",
       "      <td>[male, 15, Student, Leo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>testing testing</td>\n",
       "      <td>[male, 15, Student, Leo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>thanks yahoo toolbar capture urls popups means...</td>\n",
       "      <td>[male, 33, InvestmentBanking, Aquarius]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>interesting conversation dad morning talking k...</td>\n",
       "      <td>[male, 33, InvestmentBanking, Aquarius]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender  age              topic      sign  \\\n",
       "0   male   15            Student       Leo   \n",
       "2   male   15            Student       Leo   \n",
       "3   male   15            Student       Leo   \n",
       "4   male   33  InvestmentBanking  Aquarius   \n",
       "5   male   33  InvestmentBanking  Aquarius   \n",
       "\n",
       "                                                text  \\\n",
       "0  info found pages mb pdf files wait untill team...   \n",
       "2  het kader van kernfusie op aarde maak je eigen...   \n",
       "3                                    testing testing   \n",
       "4  thanks yahoo toolbar capture urls popups means...   \n",
       "5  interesting conversation dad morning talking k...   \n",
       "\n",
       "                                    labels  \n",
       "0                 [male, 15, Student, Leo]  \n",
       "2                 [male, 15, Student, Leo]  \n",
       "3                 [male, 15, Student, Leo]  \n",
       "4  [male, 33, InvestmentBanking, Aquarius]  \n",
       "5  [male, 33, InvestmentBanking, Aquarius]  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop gender,age,topic & sign as they are already merged to labels column\n",
    "blog_df.drop(columns=['gender','age','topic','sign'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>info found pages mb pdf files wait untill team...</td>\n",
       "      <td>[male, 15, Student, Leo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>het kader van kernfusie op aarde maak je eigen...</td>\n",
       "      <td>[male, 15, Student, Leo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>testing testing</td>\n",
       "      <td>[male, 15, Student, Leo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thanks yahoo toolbar capture urls popups means...</td>\n",
       "      <td>[male, 33, InvestmentBanking, Aquarius]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>interesting conversation dad morning talking k...</td>\n",
       "      <td>[male, 33, InvestmentBanking, Aquarius]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  info found pages mb pdf files wait untill team...   \n",
       "2  het kader van kernfusie op aarde maak je eigen...   \n",
       "3                                    testing testing   \n",
       "4  thanks yahoo toolbar capture urls popups means...   \n",
       "5  interesting conversation dad morning talking k...   \n",
       "\n",
       "                                    labels  \n",
       "0                 [male, 15, Student, Leo]  \n",
       "2                 [male, 15, Student, Leo]  \n",
       "3                 [male, 15, Student, Leo]  \n",
       "4  [male, 33, InvestmentBanking, Aquarius]  \n",
       "5  [male, 33, InvestmentBanking, Aquarius]  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate features and labels, and split the data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= blog_df.text\n",
    "y = blog_df.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# split X & y into Train and Test datasets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38180,)\n",
      "(38180,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9546,)\n",
      "(9546,)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45112    aah summer finally time straw hair one day gre...\n",
       "27710    keep degrading warzone going agree mention peo...\n",
       "5340     love fireflies lightning bugs glimpses light d...\n",
       "31110    second day thingy totally forgot username log ...\n",
       "42089    discovered joys urllink cky song episode jacka...\n",
       "                               ...                        \n",
       "987      got first round interview mars inc got email b...\n",
       "8044     urllink urllink pants originally uploaded urll...\n",
       "34374    well king paul back throne sort got minority g...\n",
       "36137    watch late night shows blind date elimidate no...\n",
       "23848    lady save twenty year olds sprout works days w...\n",
       "Name: text, Length: 9546, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Create a Bag of Words using count vectorizer\n",
    "\n",
    "i. Use ngram_range=(1, 2)\n",
    "\n",
    "ii. Vectorize training and testing features\n",
    "\n",
    "b. Print the term-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cvect = CountVectorizer(ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2395722"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorize Train data\n",
    "\n",
    "cvect.fit(X_train)\n",
    "\n",
    "#Check the vocablury size\n",
    "len(cvect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ct = cvect.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<38180x2395722 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 6841850 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x2395722 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 216 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_ct[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ct = cvect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<9546x2395722 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1202555 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa',\n",
       " 'aa aa',\n",
       " 'aa advert',\n",
       " 'aa amazing',\n",
       " 'aa anger',\n",
       " 'aa batteries',\n",
       " 'aa class',\n",
       " 'aa damn',\n",
       " 'aa ended',\n",
       " 'aa eriol']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvect.get_feature_names()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 52442)\t1\n",
      "  (0, 53466)\t1\n",
      "  (0, 56144)\t2\n",
      "  (0, 56263)\t1\n",
      "  (0, 58940)\t1\n",
      "  (0, 79043)\t3\n",
      "  (0, 79882)\t1\n",
      "  (0, 79893)\t1\n",
      "  (0, 80334)\t1\n",
      "  (0, 110337)\t1\n",
      "  (0, 110459)\t1\n",
      "  (0, 138212)\t1\n",
      "  (0, 139332)\t1\n",
      "  (0, 145086)\t2\n",
      "  (0, 148550)\t1\n",
      "  (0, 149122)\t1\n",
      "  (0, 164233)\t1\n",
      "  (0, 164289)\t1\n",
      "  (0, 241569)\t3\n",
      "  (0, 241683)\t1\n",
      "  (0, 241751)\t1\n",
      "  (0, 242078)\t1\n",
      "  (0, 293363)\t1\n",
      "  (0, 293502)\t1\n",
      "  (0, 382657)\t1\n",
      "  :\t:\n",
      "  (38179, 2119246)\t1\n",
      "  (38179, 2119760)\t1\n",
      "  (38179, 2127746)\t1\n",
      "  (38179, 2128565)\t1\n",
      "  (38179, 2144224)\t1\n",
      "  (38179, 2145413)\t1\n",
      "  (38179, 2158687)\t4\n",
      "  (38179, 2158706)\t1\n",
      "  (38179, 2158714)\t1\n",
      "  (38179, 2159018)\t2\n",
      "  (38179, 2161018)\t1\n",
      "  (38179, 2161022)\t1\n",
      "  (38179, 2172157)\t1\n",
      "  (38179, 2173131)\t1\n",
      "  (38179, 2187523)\t1\n",
      "  (38179, 2188171)\t1\n",
      "  (38179, 2229888)\t1\n",
      "  (38179, 2230912)\t1\n",
      "  (38179, 2264290)\t2\n",
      "  (38179, 2264650)\t1\n",
      "  (38179, 2264762)\t1\n",
      "  (38179, 2306195)\t1\n",
      "  (38179, 2308557)\t1\n",
      "  (38179, 2334453)\t1\n",
      "  (38179, 2334559)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X_train_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 382)\t1\n",
      "  (0, 4131)\t1\n",
      "  (0, 4155)\t1\n",
      "  (0, 50369)\t1\n",
      "  (0, 56144)\t2\n",
      "  (0, 57202)\t1\n",
      "  (0, 59110)\t1\n",
      "  (0, 59885)\t1\n",
      "  (0, 60082)\t1\n",
      "  (0, 68406)\t1\n",
      "  (0, 70307)\t1\n",
      "  (0, 70326)\t1\n",
      "  (0, 75759)\t1\n",
      "  (0, 124836)\t1\n",
      "  (0, 125272)\t1\n",
      "  (0, 140611)\t1\n",
      "  (0, 141187)\t1\n",
      "  (0, 141776)\t1\n",
      "  (0, 170078)\t2\n",
      "  (0, 170156)\t1\n",
      "  (0, 170210)\t3\n",
      "  (0, 170294)\t1\n",
      "  (0, 170726)\t1\n",
      "  (0, 205015)\t1\n",
      "  (0, 205419)\t1\n",
      "  :\t:\n",
      "  (9545, 2294843)\t1\n",
      "  (9545, 2295914)\t1\n",
      "  (9545, 2297459)\t1\n",
      "  (9545, 2297631)\t1\n",
      "  (9545, 2301325)\t1\n",
      "  (9545, 2301758)\t1\n",
      "  (9545, 2311321)\t1\n",
      "  (9545, 2311768)\t1\n",
      "  (9545, 2317581)\t1\n",
      "  (9545, 2318201)\t1\n",
      "  (9545, 2348578)\t2\n",
      "  (9545, 2349516)\t1\n",
      "  (9545, 2350211)\t1\n",
      "  (9545, 2350370)\t1\n",
      "  (9545, 2354158)\t1\n",
      "  (9545, 2354317)\t1\n",
      "  (9545, 2357505)\t1\n",
      "  (9545, 2361025)\t1\n",
      "  (9545, 2368358)\t1\n",
      "  (9545, 2375918)\t1\n",
      "  (9545, 2376912)\t1\n",
      "  (9545, 2377409)\t1\n",
      "  (9545, 2378691)\t1\n",
      "  (9545, 2382054)\t1\n",
      "  (9545, 2382215)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X_test_ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary to get the count of every label i.e. the key will be label name and value will be the total count of the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts =dict()\n",
    "\n",
    "for labels in blog_df.labels.values:\n",
    "    for label in labels:\n",
    "        if label in label_counts:\n",
    "            label_counts[str(label)]+=1\n",
    "        else:\n",
    "            label_counts[str(label)] =1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'male': 24519,\n",
       " '15': 1,\n",
       " 'Student': 10204,\n",
       " 'Leo': 3666,\n",
       " '33': 1,\n",
       " 'InvestmentBanking': 83,\n",
       " 'Aquarius': 4627,\n",
       " 'female': 23207,\n",
       " '14': 1,\n",
       " 'indUnk': 16891,\n",
       " 'Aries': 7280,\n",
       " '25': 1,\n",
       " 'Capricorn': 3663,\n",
       " '17': 1,\n",
       " 'Gemini': 2400,\n",
       " '23': 1,\n",
       " 'Non-Profit': 470,\n",
       " 'Cancer': 4400,\n",
       " 'Banking': 279,\n",
       " '37': 1,\n",
       " 'Sagittarius': 4417,\n",
       " '26': 1,\n",
       " '24': 1,\n",
       " 'Scorpio': 3100,\n",
       " '27': 1,\n",
       " 'Education': 2523,\n",
       " '45': 1,\n",
       " 'Engineering': 1341,\n",
       " 'Libra': 4186,\n",
       " 'Science': 648,\n",
       " '34': 1,\n",
       " '41': 1,\n",
       " 'Communications-Media': 1434,\n",
       " 'BusinessServices': 399,\n",
       " 'Sports-Recreation': 118,\n",
       " 'Virgo': 2758,\n",
       " 'Taurus': 3235,\n",
       " 'Arts': 1786,\n",
       " 'Pisces': 3994,\n",
       " '44': 1,\n",
       " '16': 1,\n",
       " 'Internet': 1341,\n",
       " 'Museums-Libraries': 277,\n",
       " 'Accounting': 349,\n",
       " '39': 1,\n",
       " '35': 1,\n",
       " 'Technology': 4101,\n",
       " '36': 1,\n",
       " 'Law': 290,\n",
       " '46': 1,\n",
       " 'Consulting': 193,\n",
       " 'Automotive': 116,\n",
       " '42': 1,\n",
       " 'Religion': 250,\n",
       " '13': 1,\n",
       " 'Fashion': 1749,\n",
       " '38': 1,\n",
       " '43': 1,\n",
       " 'Publishing': 203,\n",
       " '40': 1,\n",
       " 'Marketing': 395,\n",
       " 'LawEnforcement-Security': 122,\n",
       " 'HumanResources': 76,\n",
       " 'Telecommunications': 12,\n",
       " 'Military': 187,\n",
       " 'Government': 575,\n",
       " 'Transportation': 188,\n",
       " 'Architecture': 63,\n",
       " 'Advertising': 217,\n",
       " '47': 1,\n",
       " 'Agriculture': 67,\n",
       " 'Biotech': 101,\n",
       " 'RealEstate': 17,\n",
       " 'Manufacturing': 440,\n",
       " '48': 1,\n",
       " 'Construction': 26,\n",
       " 'Chemicals': 73,\n",
       " 'Maritime': 54,\n",
       " 'Tourism': 63,\n",
       " 'Environment': 5}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have noticed before, in this task each example can have multiple tags. To deal with such kind of prediction, we need to transform labels in a binary form and the prediction will be a mask of 0s and 1s. For this purpose, it is convenient to use MultiLabelBinarizer from sklearn a. Convert your train and test labels using MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "binarizer=MultiLabelBinarizer(classes=sorted(label_counts.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = binarizer.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = binarizer.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 1, 1],\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 1]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 1],\n",
       "       [0, 0, 0, ..., 1, 1, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 0]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, we will use OneVsRestClassifier class. In this approach k classifiers (= number of tags) are trained. As a basic classifier, use LogisticRegression . It is one of the simplest methods, but often it performs good enough in text classification tasks. It might take some time because the number of classifiers to train is large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the classifier, make predictions and get the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Print the following\n",
    " \n",
    " i. Accuracy score\n",
    " \n",
    " ii. F1 score\n",
    " \n",
    " iii. Average precision score\n",
    " \n",
    " iv. Average recall score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(max_iter=50))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(solver ='lbfgs', max_iter =50)\n",
    "model = OneVsRestClassifier(model)\n",
    "model.fit(X_train_ct,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ypred=model.predict(X_test_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 1],\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 0]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 1],\n",
       "       [0, 0, 0, ..., 1, 1, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 0]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Micro-average Method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is used to sum up the individual true positives, false positives, and false negatives of the system for different sets and the apply them to get the statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Macro-average Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method is straight forward. Just take the average of the precision and recall of the system on different sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def display_metrics_micro(Ytest, Ypred):\n",
    "    print('Accuracy score: ', accuracy_score(Ytest, Ypred))\n",
    "    print('F1 score: Micro', f1_score(Ytest, Ypred, average='micro'))\n",
    "    print('Average precision score: Micro', average_precision_score(Ytest, Ypred, average='micro'))\n",
    "    print('Average recall score: Micro', recall_score(Ytest, Ypred, average='micro'))\n",
    "    \n",
    "    \n",
    "def display_metrics_macro(Ytest, Ypred):\n",
    "    print('Accuracy score: ', accuracy_score(Ytest, Ypred))\n",
    "    print('F1 score: Macro', f1_score(Ytest, Ypred, average='macro'))\n",
    "    print('Average recall score: MAcro', recall_score(Ytest, Ypred, average='macro'))\n",
    "    \n",
    "def display_metrics_weighted(Ytest, Ypred):\n",
    "    print('Accuracy score: ', accuracy_score(Ytest, Ypred))\n",
    "    print('F1 score: weighted', f1_score(Ytest, Ypred, average='weighted'))\n",
    "    print('Average precision score: weighted', average_precision_score(Ytest, Ypred, average='weighted'))\n",
    "    print('Average recall score: weighted', recall_score(Ytest, Ypred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.15273412947831552\n",
      "F1 score: Micro 0.5266322825357911\n",
      "Average precision score: Micro 0.3326194649621051\n",
      "Average recall score: Micro 0.3975487115021999\n"
     ]
    }
   ],
   "source": [
    "display_metrics_micro(y_test,Ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.15273412947831552\n",
      "F1 score: Macro 0.16217090553148034\n",
      "Average recall score: MAcro 0.11144507658216643\n"
     ]
    }
   ],
   "source": [
    "display_metrics_macro(y_test,Ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.15273412947831552\n",
      "F1 score: weighted 0.4871625393828181\n",
      "Average precision score: weighted 0.4267776369972152\n",
      "Average recall score: weighted 0.3975487115021999\n"
     ]
    }
   ],
   "source": [
    "display_metrics_weighted(y_test,Ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print true label and predicted label for any five examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = Ypred[:15]\n",
    "actuals = y_test[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Gemini', 'Student', 'female'),\n",
       " ('Sagittarius', 'indUnk', 'male'),\n",
       " ('Scorpio', 'female', 'indUnk'),\n",
       " ('Cancer', 'Fashion', 'female'),\n",
       " ('Libra', 'female', 'indUnk'),\n",
       " ('Libra', 'female', 'indUnk'),\n",
       " ('Pisces', 'Technology', 'male'),\n",
       " ('Capricorn', 'Student', 'female'),\n",
       " ('Education', 'Pisces', 'male'),\n",
       " ('Aries', 'Technology', 'male'),\n",
       " ('Capricorn', 'Student', 'female'),\n",
       " ('Law', 'Taurus', 'male'),\n",
       " ('Capricorn', 'Tourism', 'male'),\n",
       " ('Pisces', 'Student', 'female'),\n",
       " ('Communications-Media', 'Leo', 'male')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "five_actual = binarizer.inverse_transform(actuals)\n",
    "five_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Student', 'female'),\n",
       " ('Sagittarius', 'indUnk', 'male'),\n",
       " ('female',),\n",
       " ('male',),\n",
       " ('female', 'indUnk'),\n",
       " ('female', 'indUnk'),\n",
       " ('Pisces', 'Technology', 'male'),\n",
       " (),\n",
       " ('Education', 'Pisces', 'male'),\n",
       " (),\n",
       " ('Student', 'female'),\n",
       " ('Taurus', 'male'),\n",
       " ('indUnk', 'male'),\n",
       " ('female',),\n",
       " ('Aquarius', 'female')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "five_pred = binarizer.inverse_transform(preds)\n",
    "five_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "def build_model_train(X_train, y_train, X_valid=None, y_valid=None, C=1.0, model='lr'):\n",
    "    if model=='lr':\n",
    "        model = LogisticRegression(C=C, penalty='l1', dual=False, solver='liblinear')\n",
    "        model = OneVsRestClassifier(model)\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "    elif model=='svm':\n",
    "        model = LinearSVC(C=C, penalty='l1', dual=False, loss='squared_hinge')\n",
    "        model = OneVsRestClassifier(model)\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "    elif model=='nbayes':\n",
    "        model = MultinomialNB(alpha=1.0)\n",
    "        model = OneVsRestClassifier(model)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "    elif model=='lda':\n",
    "        model = LinearDiscriminantAnalysis(solver='svd')\n",
    "        model = OneVsRestClassifier(model)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "**displaying  metrics for the mode OneVsRestClassifier(estimator=LogisticRegression(penalty='l1',\n",
      "                                                 solver='liblinear'))\n",
      "\n",
      "Accuracy score:  0.18856065367693275\n",
      "F1 score: Micro 0.5460306284104912\n",
      "Average precision score: Micro 0.3410631906272306\n",
      "Average recall score: Micro 0.4332704797821077\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Accuracy score:  0.18856065367693275\n",
      "F1 score: Macro 0.22652987777219838\n",
      "Average recall score: MAcro 0.1684097058474611\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Accuracy score:  0.18856065367693275\n",
      "F1 score: weighted 0.526099334034129\n",
      "Average precision score: weighted 0.4377674446152397\n",
      "Average recall score: weighted 0.4332704797821077\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "**displaying  metrics for the mode OneVsRestClassifier(estimator=LinearSVC(dual=False, penalty='l1'))\n",
      "\n",
      "Accuracy score:  0.1689712968782736\n",
      "F1 score: Micro 0.5146608315098468\n",
      "Average precision score: Micro 0.31781030628526574\n",
      "Average recall score: Micro 0.39011104127383195\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Accuracy score:  0.1689712968782736\n",
      "F1 score: Macro 0.2365769122873358\n",
      "Average recall score: MAcro 0.16814227716247113\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Accuracy score:  0.1689712968782736\n",
      "F1 score: weighted 0.49808465784985595\n",
      "Average precision score: weighted 0.42891337676773117\n",
      "Average recall score: weighted 0.39011104127383195\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "**displaying  metrics for the mode OneVsRestClassifier(estimator=MultinomialNB())\n",
      "\n",
      "Accuracy score:  0.032893358474753824\n",
      "F1 score: Micro 0.42133025218527076\n",
      "Average precision score: Micro 0.24633498432498707\n",
      "Average recall score: Micro 0.2928626300719324\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Accuracy score:  0.032893358474753824\n",
      "F1 score: Macro 0.03960389414644774\n",
      "Average recall score: MAcro 0.030394574633512954\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Accuracy score:  0.032893358474753824\n",
      "F1 score: weighted 0.3274164495441221\n",
      "Average precision score: weighted 0.34879784647255835\n",
      "Average recall score: weighted 0.2928626300719324\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = ['lr','svm','nbayes']\n",
    "for model in models:\n",
    "    model = build_model_train(X_train_ct,y_train,model=model)\n",
    "    model.fit(X_train_ct,y_train)\n",
    "    Ypred=model.predict(X_test_ct)\n",
    "    print(\"\\n\")\n",
    "    print(f\"**displaying  metrics for the mode {model}\\n\")\n",
    "    display_metrics_micro(y_test,Ypred)\n",
    "    print(\"\\n\")\n",
    "    print(\"\\n\")\n",
    "    display_metrics_macro(y_test,Ypred)\n",
    "    print(\"\\n\")\n",
    "    print(\"\\n\")\n",
    "    display_metrics_weighted(y_test,Ypred)\n",
    "    print(\"\\n\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Improve Performance of model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4A.Experiment with other vectorisers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use TF-IDF vectorizer to see if that helps improve the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_vector(data):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    vect = tfidf_vectorizer.fit_transform(data)\n",
    "    return vect, tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf, tfidf_vectorizer = tfidf_vector(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<38180x111502 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3011265 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x111502 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 91 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4B. Build classifier Models using other algorithms than base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=RandomForestClassifier())"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators =100, criterion ='gini', max_depth =None)\n",
    "model = OneVsRestClassifier(model)\n",
    "model.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "**displaying  metrics for the mode OneVsRestClassifier(estimator=RandomForestClassifier())\n",
      "\n",
      "Accuracy score:  0.11030798240100566\n",
      "F1 score: Micro 0.47564290697394696\n",
      "Average precision score: Micro 0.28847121000428444\n",
      "Average recall score: Micro 0.34520567078706615\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Accuracy score:  0.11030798240100566\n",
      "F1 score: Macro 0.12267122139174329\n",
      "Average recall score: MAcro 0.08229713217562458\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Accuracy score:  0.11030798240100566\n",
      "F1 score: weighted 0.4222863709442582\n",
      "Average precision score: weighted 0.3965503399453171\n",
      "Average recall score: weighted 0.34520567078706615\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Ypred=model.predict(X_test_tfidf)\n",
    "print(\"\\n\")\n",
    "print(f\"**displaying  metrics for the mode {model}\\n\")\n",
    "display_metrics_micro(y_test,Ypred)\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "display_metrics_macro(y_test,Ypred)\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "display_metrics_weighted(y_test,Ypred)\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4C.Tune Parameters/Hyperparameters of the model/s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=RandomForestClassifier(criterion='entropy',\n",
       "                                                     n_estimators=10))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's tune the parameters of the model to see if this helps to improve the model performance\n",
    "\n",
    "model = RandomForestClassifier(n_estimators =10, criterion ='entropy', max_depth =None)\n",
    "model = OneVsRestClassifier(model)\n",
    "model.fit(X_train_tfidf, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4D. Clearly print Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "**displaying  metrics for the mode OneVsRestClassifier(estimator=RandomForestClassifier(criterion='entropy',\n",
      "                                                     n_estimators=10))\n",
      "\n",
      "Accuracy score:  0.04923528179342133\n",
      "F1 score: Micro 0.38891143349903623\n",
      "Average precision score: Micro 0.21769386649079842\n",
      "Average recall score: Micro 0.2677212095816747\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Accuracy score:  0.04923528179342133\n",
      "F1 score: Macro 0.07403803364656894\n",
      "Average recall score: MAcro 0.04837467547351254\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Accuracy score:  0.04923528179342133\n",
      "F1 score: weighted 0.33620644171651504\n",
      "Average precision score: weighted 0.3410020734821616\n",
      "Average recall score: weighted 0.2677212095816747\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Ypred=model.predict(X_test_tfidf)\n",
    "print(\"\\n\")\n",
    "print(f\"**displaying  metrics for the mode {model}\\n\")\n",
    "display_metrics_micro(y_test,Ypred)\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "display_metrics_macro(y_test,Ypred)\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "display_metrics_weighted(y_test,Ypred)\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.Share insights on relative performance comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5A. Which vectorizer performed better? Probable reason?."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "I think TF-IDF is better than Count Vectorizers because it not only focuses on the frequency of words present in the corpus but also provides the importance of the words. We can then remove the words that are less important for analysis, hence making the model building less complex by reducing the input dimensions.The only difference is that the TfidfVectorizer() returns floats while the CountVectorizer() returns ints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5B. Which model outperformed? Probable reason?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistics Regression model performed best amongst all models. As evidenced above, LR model had the best accoracy and recall scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5C. Which parameter/hyperparameter significantly helped\n",
    "to improve performance?Probable reason?."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example for RandomForest Classifier, the parameter \"n_estimators\" was key to help improve the performance of the model. This is the number of trees we want to build before taking the maximum voting or averages of predictions. Higher number of trees gives us better performance but makes our code slower. We should choose as high value as our processor can handle because this makes our predictions stronger and more stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5D. According to you, which performance metric should be\n",
    "given most importance, why?."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there are many ways for measuring classification performance,but the key classification metrics are Accuracy, Recall, Precision and F1-score. And out of these, I think F1 score is the most important metric as it combines precision and recall into one metric. This is the harmonic mean of precision and recall, and is probably the most used metric for evaluating binary classification models. If our F1 score increases, it means that our model has increased performance for accuracy, recall or both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a> PART B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a> **DOMAIN**: Customer support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a> **CONTEXT**: Great Learning has a an academic support department which receives numerous support requests every day throughout the year.Teams are spread across geographies and try to provide support round the year. Sometimes there are circumstances where due to heavy workload certain request resolutions are delayed, impacting company’s business. Some of the requests are very generic where a proper resolution procedure delivered to the user can solve the problem. Company is looking forward to design an automation which can interact with the user, understand the problem and display the resolution procedure [ if found as a generic request ] or redirect the request to an actual human support executive if the request is complex or not in it’s database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a> **DATA DESCRIPTION**: A sample corpus is attached for your reference. Please enhance/add more data to the corpus using your linguistics skills."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a> **PROJECT OBJECTIVE**: Design a python based interactive semi - rule based chatbot which can do the following:\n",
    "1. Start chat session with greetings and ask what the user is looking for. [5 Marks]\n",
    "2. Accept dynamic text based questions from the user. Reply back with relevant answer from the designed corpus. [10 Marks]\n",
    "3. End the chat session only if the user requests to end else ask what the user is looking for. Loop continues till the user asks to end it. [5 Marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our data is in JSON format, we’ll need to parse our “GL Bot.json” into Python language. This can be done using the JSON package(we have already imported it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = open('GL Bot.json').read()\n",
    "intents =json.loads(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intents': [{'tag': 'Intro',\n",
       "   'patterns': ['hi',\n",
       "    'how are you',\n",
       "    'is anyone there',\n",
       "    'hello',\n",
       "    'whats up',\n",
       "    'hey',\n",
       "    'yo',\n",
       "    'listen',\n",
       "    'please help me',\n",
       "    'i am learner from',\n",
       "    'i belong to',\n",
       "    'aiml batch',\n",
       "    'aifl batch',\n",
       "    'i am from',\n",
       "    'my pm is',\n",
       "    'blended',\n",
       "    'online',\n",
       "    'i am from',\n",
       "    'hey ya',\n",
       "    'talking to you for first time'],\n",
       "   'responses': ['Hello! how can i help you ?'],\n",
       "   'context_set': ''},\n",
       "  {'tag': 'Exit',\n",
       "   'patterns': ['thank you',\n",
       "    'thanks',\n",
       "    'cya',\n",
       "    'see you',\n",
       "    'later',\n",
       "    'see you later',\n",
       "    'goodbye',\n",
       "    'i am leaving',\n",
       "    'have a Good day',\n",
       "    'you helped me',\n",
       "    'thanks a lot',\n",
       "    'thanks a ton',\n",
       "    'you are the best',\n",
       "    'great help',\n",
       "    'too good',\n",
       "    'you are a good learning buddy'],\n",
       "   'responses': ['I hope I was able to assist you, Good Bye'],\n",
       "   'context_set': ''},\n",
       "  {'tag': 'Olympus',\n",
       "   'patterns': ['olympus',\n",
       "    'explain me how olympus works',\n",
       "    'I am not able to understand olympus',\n",
       "    'olympus window not working',\n",
       "    'no access to olympus',\n",
       "    'unable to see link in olympus',\n",
       "    'no link visible on olympus',\n",
       "    'whom to contact for olympus',\n",
       "    'lot of problem with olympus',\n",
       "    'olypus is not a good tool',\n",
       "    'lot of problems with olympus',\n",
       "    'how to use olympus',\n",
       "    'teach me olympus'],\n",
       "   'responses': ['Link: Olympus wiki'],\n",
       "   'context_set': ''},\n",
       "  {'tag': 'SL',\n",
       "   'patterns': ['i am not able to understand svm',\n",
       "    'explain me how machine learning works',\n",
       "    'i am not able to understand naive bayes',\n",
       "    'i am not able to understand logistic regression',\n",
       "    'i am not able to understand ensemble techb=niques',\n",
       "    'i am not able to understand knn',\n",
       "    'i am not able to understand knn imputer',\n",
       "    'i am not able to understand cross validation',\n",
       "    'i am not able to understand boosting',\n",
       "    'i am not able to understand random forest',\n",
       "    'i am not able to understand ada boosting',\n",
       "    'i am not able to understand gradient boosting',\n",
       "    'machine learning',\n",
       "    'ML',\n",
       "    'SL',\n",
       "    'supervised learning',\n",
       "    'knn',\n",
       "    'logistic regression',\n",
       "    'regression',\n",
       "    'classification',\n",
       "    'naive bayes',\n",
       "    'nb',\n",
       "    'ensemble techniques',\n",
       "    'bagging',\n",
       "    'boosting',\n",
       "    'ada boosting',\n",
       "    'ada',\n",
       "    'gradient boosting',\n",
       "    'hyper parameters'],\n",
       "   'responses': ['Link: Machine Learning wiki '],\n",
       "   'context_set': ''},\n",
       "  {'tag': 'NN',\n",
       "   'patterns': ['what is deep learning',\n",
       "    'unable to understand deep learning',\n",
       "    'explain me how deep learning works',\n",
       "    'i am not able to understand deep learning',\n",
       "    'not able to understand neural nets',\n",
       "    'very diffult to understand neural nets',\n",
       "    'unable to understand neural nets',\n",
       "    'ann',\n",
       "    'artificial intelligence',\n",
       "    'artificial neural networks',\n",
       "    'weights',\n",
       "    'activation function',\n",
       "    'hidden layers',\n",
       "    'softmax',\n",
       "    'sigmoid',\n",
       "    'relu',\n",
       "    'otimizer',\n",
       "    'forward propagation',\n",
       "    'backward propagation',\n",
       "    'epochs',\n",
       "    'epoch',\n",
       "    'what is an epoch',\n",
       "    'adam',\n",
       "    'sgd'],\n",
       "   'responses': ['Link: Neural Nets wiki'],\n",
       "   'context_set': ''},\n",
       "  {'tag': 'Bot',\n",
       "   'patterns': ['what is your name',\n",
       "    'who are you',\n",
       "    'name please',\n",
       "    'when are your hours of opertions',\n",
       "    'what are your working hours',\n",
       "    'hours of operation',\n",
       "    'working hours',\n",
       "    'hours'],\n",
       "   'responses': ['I am your virtual learning assistant'],\n",
       "   'context_set': ''},\n",
       "  {'tag': 'Profane',\n",
       "   'patterns': ['what the hell',\n",
       "    'bloody stupid bot',\n",
       "    'do you think you are very smart',\n",
       "    'screw you',\n",
       "    'i hate you',\n",
       "    'you are stupid',\n",
       "    'jerk',\n",
       "    'you are a joke',\n",
       "    'useless piece of shit'],\n",
       "   'responses': ['Please use respectful words'],\n",
       "   'context_set': ''},\n",
       "  {'tag': 'Ticket',\n",
       "   'patterns': ['my problem is not solved',\n",
       "    'you did not help me',\n",
       "    'not a good solution',\n",
       "    'bad solution',\n",
       "    'not good solution',\n",
       "    'no help',\n",
       "    'wasted my time',\n",
       "    'useless bot',\n",
       "    'create a ticket'],\n",
       "   'responses': ['Tarnsferring the request to your PM'],\n",
       "   'context_set': ''}]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 documents\n",
      "8 classes ['Bot', 'Exit', 'Intro', 'NN', 'Olympus', 'Profane', 'SL', 'Ticket']\n",
      "158 unique lemmatized words ['a', 'able', 'access', 'activation', 'ada', 'adam', 'aifl', 'aiml', 'am', 'an', 'ann', 'anyone', 'are', 'artificial', 'backward', 'bad', 'bagging', 'batch', 'bayes', 'belong', 'best', 'blended', 'bloody', 'boosting', 'bot', 'buddy', 'classification', 'contact', 'create', 'cross', 'cya', 'day', 'deep', 'did', 'diffult', 'do', 'ensemble', 'epoch', 'explain', 'first', 'for', 'forest', 'forward', 'from', 'function', 'good', 'goodbye', 'gradient', 'great', 'hate', 'have', 'hell', 'hello', 'help', 'helped', 'hey', 'hi', 'hidden', 'hour', 'how', 'hyper', 'i', 'imputer', 'in', 'intelligence', 'is', 'jerk', 'joke', 'knn', 'later', 'layer', 'learner', 'learning', 'leaving', 'link', 'listen', 'logistic', 'lot', 'machine', 'me', 'ml', 'my', 'naive', 'name', 'nb', 'net', 'network', 'neural', 'no', 'not', 'of', 'olympus', 'olypus', 'on', 'online', 'operation', 'opertions', 'otimizer', 'parameter', 'piece', 'please', 'pm', 'problem', 'propagation', 'random', 'regression', 'relu', 'screw', 'see', 'sgd', 'shit', 'sigmoid', 'sl', 'smart', 'softmax', 'solution', 'solved', 'stupid', 'supervised', 'svm', 'talking', 'teach', 'techb=niques', 'technique', 'thank', 'thanks', 'the', 'there', 'think', 'ticket', 'time', 'to', 'ton', 'too', 'tool', 'unable', 'understand', 'up', 'use', 'useless', 'validation', 'very', 'visible', 'wasted', 'weight', 'what', 'whats', 'when', 'who', 'whom', 'window', 'with', 'work', 'working', 'ya', 'yo', 'you', 'your']\n"
     ]
    }
   ],
   "source": [
    "#creating lists \n",
    "words=[]\n",
    "classes = []\n",
    "documents = []\n",
    "#ignore these words\n",
    "ignore_words = ['?', '!']\n",
    "\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "\n",
    "        #tokenization technique\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        words.extend(w)\n",
    "        #adding documents\n",
    "        documents.append((w, intent['tag']))\n",
    "\n",
    "        # append the tags into \"classes\" list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "\n",
    "# lemmatization technique\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
    "# this way we can remove duplicates\n",
    "words = sorted(list(set(words)))\n",
    "# now sort the \"classes\" list\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "print (len(documents), \"documents\")\n",
    "# classes are categories of intents\n",
    "print (len(classes), \"classes\", classes)\n",
    "# print all words after apply the two techniques\n",
    "print (len(words), \"unique lemmatized words\", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data created\n"
     ]
    }
   ],
   "source": [
    "# creating training data\n",
    "training = []\n",
    "# empty array for output\n",
    "output_empty = [0] * len(classes)\n",
    "# training set\n",
    "for doc in documents:\n",
    "    # initialize the list \"bag\"(which is going to be bag of words)\n",
    "    bag = []\n",
    "    # creating list for tokens of pattern(words)\n",
    "    pattern_words = doc[0]\n",
    "    # lemmatization\n",
    "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
    "    # if the word is found in current pattern then append 1 in the bag of words array otherwise append 0\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "    \n",
    "    # only for current tag, output will be 1. Otherwise 0\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "    \n",
    "    training.append([bag, output_row])\n",
    "    \n",
    "# shuffling the features\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "\n",
    "#spliting the data into x and y . X - patterns, Y - intents\n",
    "train_x = list(training[:,0])\n",
    "train_y = list(training[:,1])\n",
    "print(\"Training data created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Keras sequential API to build a deep neural network that has 3 layers. \n",
    "\n",
    "Compile this Keras model with an SGD optimizer.\n",
    "\n",
    "Fit the model(I trained my model for 200 epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 2.0417 - accuracy: 0.1875\n",
      "Epoch 2/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1.9119 - accuracy: 0.2656\n",
      "Epoch 3/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 1.8429 - accuracy: 0.2812\n",
      "Epoch 4/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 1.7481 - accuracy: 0.3594\n",
      "Epoch 5/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 1.6809 - accuracy: 0.3984\n",
      "Epoch 6/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 1.6533 - accuracy: 0.3906\n",
      "Epoch 7/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 1.4631 - accuracy: 0.4844\n",
      "Epoch 8/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 1.3332 - accuracy: 0.5547\n",
      "Epoch 9/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 1.1977 - accuracy: 0.6250\n",
      "Epoch 10/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 1.0932 - accuracy: 0.6094\n",
      "Epoch 11/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.9735 - accuracy: 0.6641\n",
      "Epoch 12/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.8665 - accuracy: 0.7578\n",
      "Epoch 13/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.8010 - accuracy: 0.7344\n",
      "Epoch 14/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.6905 - accuracy: 0.7656\n",
      "Epoch 15/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.6027 - accuracy: 0.8203\n",
      "Epoch 16/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.5946 - accuracy: 0.8281\n",
      "Epoch 17/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.5314 - accuracy: 0.8516\n",
      "Epoch 18/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.4937 - accuracy: 0.8594\n",
      "Epoch 19/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.4364 - accuracy: 0.8828\n",
      "Epoch 20/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.3607 - accuracy: 0.9062\n",
      "Epoch 21/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.4070 - accuracy: 0.8906\n",
      "Epoch 22/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.3499 - accuracy: 0.8906\n",
      "Epoch 23/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2725 - accuracy: 0.9141\n",
      "Epoch 24/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2334 - accuracy: 0.9688\n",
      "Epoch 25/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.2216 - accuracy: 0.9453\n",
      "Epoch 26/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2004 - accuracy: 0.9531\n",
      "Epoch 27/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2584 - accuracy: 0.9141\n",
      "Epoch 28/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1947 - accuracy: 0.9375\n",
      "Epoch 29/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.2590 - accuracy: 0.9219\n",
      "Epoch 30/200\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.2419 - accuracy: 0.9297\n",
      "Epoch 31/200\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.1521 - accuracy: 0.9766\n",
      "Epoch 32/200\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.1526 - accuracy: 0.9766\n",
      "Epoch 33/200\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.1715 - accuracy: 0.9453\n",
      "Epoch 34/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1320 - accuracy: 0.9453\n",
      "Epoch 35/200\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.1380 - accuracy: 0.9766\n",
      "Epoch 36/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1108 - accuracy: 0.9766\n",
      "Epoch 37/200\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.1519 - accuracy: 0.9688\n",
      "Epoch 38/200\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.1607 - accuracy: 0.9609\n",
      "Epoch 39/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0753 - accuracy: 0.9766\n",
      "Epoch 40/200\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.0867 - accuracy: 0.9609\n",
      "Epoch 41/200\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.1003 - accuracy: 0.9766\n",
      "Epoch 42/200\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.1405 - accuracy: 0.9453\n",
      "Epoch 43/200\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.0675 - accuracy: 0.9922\n",
      "Epoch 44/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0840 - accuracy: 0.9688\n",
      "Epoch 45/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0764 - accuracy: 0.9922\n",
      "Epoch 46/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0689 - accuracy: 0.9844\n",
      "Epoch 47/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0748 - accuracy: 0.9766\n",
      "Epoch 48/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1050 - accuracy: 0.9609\n",
      "Epoch 49/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0496 - accuracy: 1.0000\n",
      "Epoch 50/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0364 - accuracy: 1.0000\n",
      "Epoch 51/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0582 - accuracy: 0.9844\n",
      "Epoch 52/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0445 - accuracy: 0.9922\n",
      "Epoch 53/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0700 - accuracy: 0.9766\n",
      "Epoch 54/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0714 - accuracy: 0.9766\n",
      "Epoch 55/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0599 - accuracy: 0.9766\n",
      "Epoch 56/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0592 - accuracy: 0.9844\n",
      "Epoch 57/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0759 - accuracy: 0.9766\n",
      "Epoch 58/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0329 - accuracy: 1.0000\n",
      "Epoch 59/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0533 - accuracy: 0.9844\n",
      "Epoch 60/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0472 - accuracy: 1.0000\n",
      "Epoch 61/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0477 - accuracy: 0.9922\n",
      "Epoch 62/200\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.0605 - accuracy: 0.9844\n",
      "Epoch 63/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0526 - accuracy: 0.9922\n",
      "Epoch 64/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0834 - accuracy: 0.9609\n",
      "Epoch 65/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0903 - accuracy: 0.9688\n",
      "Epoch 66/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0667 - accuracy: 0.9688\n",
      "Epoch 67/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0495 - accuracy: 0.9922\n",
      "Epoch 68/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0560 - accuracy: 0.9766\n",
      "Epoch 69/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0442 - accuracy: 0.9844\n",
      "Epoch 70/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0469 - accuracy: 0.9844\n",
      "Epoch 71/200\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 0.0324 - accuracy: 0.9922\n",
      "Epoch 72/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0910 - accuracy: 0.9766\n",
      "Epoch 73/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0317 - accuracy: 1.0000\n",
      "Epoch 74/200\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.0377 - accuracy: 0.9844\n",
      "Epoch 75/200\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.0414 - accuracy: 0.9844\n",
      "Epoch 76/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0394 - accuracy: 0.9922\n",
      "Epoch 77/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0373 - accuracy: 0.9922\n",
      "Epoch 78/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0151 - accuracy: 1.0000\n",
      "Epoch 79/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0390 - accuracy: 0.9922\n",
      "Epoch 80/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0213 - accuracy: 1.0000\n",
      "Epoch 81/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0689 - accuracy: 0.9766\n",
      "Epoch 82/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0552 - accuracy: 0.9766\n",
      "Epoch 83/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0228 - accuracy: 1.0000\n",
      "Epoch 84/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0597 - accuracy: 0.9766\n",
      "Epoch 85/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0628 - accuracy: 0.9844\n",
      "Epoch 86/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0486 - accuracy: 0.9844\n",
      "Epoch 87/200\n",
      "26/26 [==============================] - 0s 880us/step - loss: 0.0122 - accuracy: 1.0000\n",
      "Epoch 88/200\n",
      "26/26 [==============================] - 0s 920us/step - loss: 0.0231 - accuracy: 1.0000\n",
      "Epoch 89/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0579 - accuracy: 0.9922\n",
      "Epoch 90/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0496 - accuracy: 0.9844\n",
      "Epoch 91/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0280 - accuracy: 0.9922\n",
      "Epoch 92/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0223 - accuracy: 1.0000\n",
      "Epoch 93/200\n",
      "26/26 [==============================] - 0s 960us/step - loss: 0.0287 - accuracy: 0.9922\n",
      "Epoch 94/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0207 - accuracy: 1.0000\n",
      "Epoch 95/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0461 - accuracy: 0.9844\n",
      "Epoch 96/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0243 - accuracy: 1.0000\n",
      "Epoch 97/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0162 - accuracy: 0.9922\n",
      "Epoch 98/200\n",
      "26/26 [==============================] - 0s 960us/step - loss: 0.0506 - accuracy: 0.9922\n",
      "Epoch 99/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0235 - accuracy: 1.0000\n",
      "Epoch 100/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0307 - accuracy: 0.9922\n",
      "Epoch 101/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0283 - accuracy: 0.9922\n",
      "Epoch 102/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0397 - accuracy: 0.9844\n",
      "Epoch 103/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0291 - accuracy: 0.9844\n",
      "Epoch 104/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0321 - accuracy: 0.9922\n",
      "Epoch 105/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0138 - accuracy: 1.0000\n",
      "Epoch 106/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0164 - accuracy: 1.0000\n",
      "Epoch 107/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0113 - accuracy: 1.0000\n",
      "Epoch 108/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0361 - accuracy: 0.9922\n",
      "Epoch 109/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0279 - accuracy: 0.9922\n",
      "Epoch 110/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0471 - accuracy: 0.9922\n",
      "Epoch 111/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0127 - accuracy: 1.0000\n",
      "Epoch 112/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0162 - accuracy: 1.0000\n",
      "Epoch 113/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0316 - accuracy: 0.9844\n",
      "Epoch 114/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0169 - accuracy: 1.0000\n",
      "Epoch 115/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0296 - accuracy: 0.9922\n",
      "Epoch 116/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0402 - accuracy: 0.9922\n",
      "Epoch 117/200\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.0222 - accuracy: 1.0000\n",
      "Epoch 118/200\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.0159 - accuracy: 1.0000\n",
      "Epoch 119/200\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.0137 - accuracy: 1.0000\n",
      "Epoch 120/200\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.0105 - accuracy: 1.0000\n",
      "Epoch 121/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0113 - accuracy: 1.0000\n",
      "Epoch 122/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0234 - accuracy: 0.9922\n",
      "Epoch 123/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0291 - accuracy: 0.9922\n",
      "Epoch 124/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0179 - accuracy: 0.9922\n",
      "Epoch 125/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0294 - accuracy: 1.0000\n",
      "Epoch 126/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 127/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0153 - accuracy: 1.0000\n",
      "Epoch 128/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0225 - accuracy: 0.9922\n",
      "Epoch 129/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0225 - accuracy: 0.9922\n",
      "Epoch 130/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0092 - accuracy: 1.0000\n",
      "Epoch 131/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0196 - accuracy: 0.9922\n",
      "Epoch 132/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0110 - accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0151 - accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0090 - accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0156 - accuracy: 0.9922\n",
      "Epoch 136/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0272 - accuracy: 0.9844\n",
      "Epoch 137/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0293 - accuracy: 0.9922\n",
      "Epoch 139/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0121 - accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0197 - accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0346 - accuracy: 0.9844\n",
      "Epoch 142/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0257 - accuracy: 0.9844\n",
      "Epoch 143/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0142 - accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0228 - accuracy: 0.9922\n",
      "Epoch 146/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0222 - accuracy: 0.9922\n",
      "Epoch 148/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0149 - accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0353 - accuracy: 0.9844\n",
      "Epoch 153/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0071 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0116 - accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0241 - accuracy: 0.9844\n",
      "Epoch 157/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0129 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0196 - accuracy: 0.9922\n",
      "Epoch 159/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0341 - accuracy: 0.9844\n",
      "Epoch 160/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0089 - accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0148 - accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0115 - accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0322 - accuracy: 0.9844\n",
      "Epoch 164/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0212 - accuracy: 0.9844\n",
      "Epoch 165/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0237 - accuracy: 0.9922\n",
      "Epoch 166/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0164 - accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0220 - accuracy: 0.9922\n",
      "Epoch 170/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0069 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0101 - accuracy: 0.9922\n",
      "Epoch 172/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0192 - accuracy: 0.9844\n",
      "Epoch 173/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0085 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0085 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0121 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0240 - accuracy: 0.9844\n",
      "Epoch 178/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0340 - accuracy: 0.9922\n",
      "Epoch 179/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0341 - accuracy: 0.9922\n",
      "Epoch 180/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0334 - accuracy: 0.9844\n",
      "Epoch 181/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0113 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0229 - accuracy: 0.9922\n",
      "Epoch 184/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0416 - accuracy: 0.9766\n",
      "Epoch 186/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0291 - accuracy: 0.9922\n",
      "Epoch 187/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0123 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0184 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0130 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0187 - accuracy: 0.9922\n",
      "Epoch 192/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0338 - accuracy: 0.9844\n",
      "Epoch 194/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0149 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0226 - accuracy: 0.9922\n",
      "Epoch 196/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0262 - accuracy: 0.9922\n",
      "Epoch 197/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0585 - accuracy: 0.9844\n",
      "Epoch 198/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0497 - accuracy: 0.9922\n",
      "Epoch 199/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0122 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0745 - accuracy: 0.9844\n",
      "model created\n"
     ]
    }
   ],
   "source": [
    "# We use Keras sequential API to build a deep neural network that has 3 layers. \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
    "\n",
    "#Compile this Keras model with SGD optimizer.\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "#fit the model with 200 epochs \n",
    "hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\n",
    "\n",
    "#print the statment when the model training is finished\n",
    "print(\"model created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing the input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input given by the user in the chatbot should be in the same manner as our model is trained on. Therefore we do similar text-preprocessing here also by tokenization and lemmatization. We are creating a function for this here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_sentence(sentence):\n",
    "    # tokenization\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    # lematization\n",
    "    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a function that can translate the user’s message(sentences) into the bag of words(array which contains 0 and 1 values). When this function finds a word from the sentence in chatbot vocabulary, it sets 1 into the corresponding position within the array. This array is going to be sent to be classified by the model to spot to what intent it belongs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow(sentence, words, show_details=True):\n",
    "    # tokeniziation(using the function we created earlier)\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    # bag of words\n",
    "    bag = [0]*len(words)  \n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s: \n",
    "                \n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag: %s\" % w)\n",
    "    #return array of nag of words\n",
    "    return(np.array(bag))\n",
    "\n",
    "#function for prediction\n",
    "def predict_class(sentence, model):\n",
    "    # filtering the prediction based on threshold value\n",
    "    p = bow(sentence, words,show_details=False)\n",
    "    res = model.predict(np.array([p]))[0]\n",
    "    #setting threshold value\n",
    "    ERROR_THRESHOLD = 0.25\n",
    "    results = [[i,r] for i,r in enumerate(res) if r>ERROR_THRESHOLD]\n",
    "    # sort on the basis of probability\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the response from the intents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating functions that can get a random response from the responses list from the identified intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choosing the response randomly from the predefined reponses for the given identified intent\n",
    "def getResponse(ints, intents_json):\n",
    "    tag = ints[0]['intent']\n",
    "    list_of_intents = intents_json['intents']\n",
    "    for i in list_of_intents:\n",
    "        if(i['tag']== tag):\n",
    "            result = random.choice(i['responses'])\n",
    "            break\n",
    "    return result\n",
    "\n",
    "#function to return the response as output in the window\n",
    "def chatbot_response(msg):\n",
    "    ints = predict_class(msg, model)\n",
    "    res = getResponse(ints, intents)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type 'bye' \n",
      "hey\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Hello! how can i help you ?\n",
      "how are you \n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Hello! how can i help you ?\n",
      "I am not able to understand olympus\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "Link: Olympus wiki\n",
      "who to contact to get for olympus please\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Link: Olympus wiki\n",
      "i am not able to understand SVM\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Link: Machine Learning wiki \n",
      "what is Deep LEARNING\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "Link: Neural Nets wiki\n",
      "your name please\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "I am your virtual learning assistant\n",
      "my problem is not resolved\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "Tarnsferring the request to your PM\n",
      "This is not a good solution\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Tarnsferring the request to your PM\n",
      "what the hell man\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "Please use respectful words\n",
      "you are useless\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "Please use respectful words\n",
      "bye\n",
      "ROBO: Bye! take care..\n"
     ]
    }
   ],
   "source": [
    "flag=True\n",
    "print(\"ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type 'bye' \")\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            flag=False\n",
    "            print(\"ROBO: You are welcome..\")\n",
    "\n",
    "        else:\n",
    "            print(\"ROBO: \",end=\"\")\n",
    "            print(chatbot_response(user_response))\n",
    "            \n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"ROBO: Bye! take care..\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
